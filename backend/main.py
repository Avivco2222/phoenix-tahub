from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import sqlite3
import shutil
import os
import uuid
from datetime import datetime
import json
import hashlib
import io

app = FastAPI()

# ==========================================
# ×× ×•×¢ ×§×œ×™×˜×ª × ×ª×•× ×™ ATS, ××‘×˜×—×ª ××™×“×¢ ×•××™×›×•×ª × ×ª×•× ×™×
# ==========================================

def mask_sensitive_data(df):
    """
    ×× ×’× ×•×Ÿ InfoSec: ××–×”×” ×¢××•×“×•×ª ×¨×’×™×©×•×ª (PII) ×œ×¤×™ ×©×,
    ×•××•×—×§ ××ª ×”××™×“×¢ ×”×¨×’×™×©. ×‘××§×•××• ××™×™×¦×¨ Hash (×”×¦×¤× ×” ×—×“-×›×™×•×•× ×™×ª) 
    ×›×“×™ ×œ××¤×©×¨ ×–×™×”×•×™ ××•×¢××“×™× ×—×•×–×¨×™× ××‘×œ×™ ×œ×©××•×¨ ××ª ×ª×¢×•×“×ª ×”×–×”×•×ª ×©×œ×”×.
    """
    sensitive_keywords = ['×ª.×–', '×ª×¢×•×“×ª ×–×”×•×ª', 'id', '×˜×œ×¤×•×Ÿ', '× ×™×™×“', 'phone', '×›×ª×•×‘×ª']
    
    for col in df.columns:
        col_lower = str(col).lower()
        if any(keyword in col_lower for keyword in sensitive_keywords):
            # ×”×¤×™×›×ª ×”××™×“×¢ ×œ-Hash ×××•×‘×˜×— (SHA-256)
            df[col] = df[col].astype(str).apply(
                lambda x: hashlib.sha256(x.encode()).hexdigest()[:12] if pd.notnull(x) and str(x).lower() not in ['nan', 'none', ''] else None
            )
            # ×©×™× ×•×™ ×©× ×”×¢××•×“×” ×›×“×™ ×œ×”×‘×”×™×¨ ×©×”×™× ×”×•×©×—×¨×”
            df.rename(columns={col: f"{col}_MASKED_SECURE"}, inplace=True)
            
    return df

@app.post("/upload/{file_type}")
async def upload_typed_file(file_type: str, file: UploadFile = File(...)):
    """× ×ª×™×‘ ×”×¢×œ××” ×¨×‘-×¡×•×’×™: candidates, jobs, hires, diversity, headcount, budget, attrition"""
    valid_types = [
        "candidates", "jobs", "hires", "diversity",
        "headcount", "budget", "attrition"
    ]

    if file_type not in valid_types:
        return {"status": "error", "message": f"Invalid file type: {file_type}. Valid: {valid_types}"}

    return {
        "status": "success",
        "file_type": file_type,
        "filename": file.filename,
        "rows_processed": 150,
        "last_updated": "×¢×›×©×™×•"
    }

ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

DB_PATH = "phoenix_enterprise.db"  # ××¡×“ × ×ª×•× ×™× ×—×“×© ×œ×’××¨×™ ×›×“×™ ×œ× ×œ×”×ª× ×’×© ×‘×™×©×Ÿ

# ==========================================
# 1. ENTITY RELATIONSHIP MODEL (×™×¦×™×¨×ª ×”×˜×‘×œ××•×ª)
# ==========================================
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()

    # --- ×˜×‘×œ××•×ª ATS ×§×™×™××•×ª ---
    c.execute('''CREATE TABLE IF NOT EXISTS candidates (id TEXT PRIMARY KEY, name TEXT, email TEXT UNIQUE, phone TEXT, source TEXT)''')
    c.execute('''CREATE TABLE IF NOT EXISTS jobs (id TEXT PRIMARY KEY, job_title TEXT UNIQUE, department TEXT, hiring_manager TEXT)''')
    c.execute('''CREATE TABLE IF NOT EXISTS applications (app_id TEXT PRIMARY KEY, candidate_id TEXT, job_id TEXT, status TEXT, recruiter TEXT, start_date TIMESTAMP, days_in_process INTEGER, upload_log_id TEXT, FOREIGN KEY(candidate_id) REFERENCES candidates(id), FOREIGN KEY(job_id) REFERENCES jobs(id))''')
    c.execute('''CREATE TABLE IF NOT EXISTS data_logs (log_id TEXT PRIMARY KEY, filename TEXT, upload_date TIMESTAMP, rows_processed INTEGER, status TEXT)''')

    # --- ×˜×‘×œ××•×ª FinOps (××©×•×“×¨×’!) ---
    c.execute('''CREATE TABLE IF NOT EXISTS finops_invoices (
                 id TEXT PRIMARY KEY, vendor TEXT, date TEXT, due_date TEXT, budget_month TEXT,
                 amount REAL, category TEXT, subcategory TEXT, status TEXT, 
                 note TEXT, file_url TEXT)''')
                 
    c.execute('''CREATE TABLE IF NOT EXISTS finops_vendors (
                 id TEXT PRIMARY KEY, name TEXT UNIQUE, default_category TEXT, 
                 total_paid REAL, active_invoices INTEGER)''')
                 
    c.execute('''CREATE TABLE IF NOT EXISTS finops_categories (
                 id INTEGER PRIMARY KEY, name TEXT UNIQUE, 
                 target REAL, previous_year_spend REAL, code TEXT, notes TEXT, subcategories TEXT)''')

    conn.commit()
    conn.close()

init_db()

# ==========================================
# ××™×œ×•×Ÿ × ×•×¨××œ×™×–×¦×™×” (Standardization Dictionary)
# ==========================================
DEPT_NORMALIZATION = {
    "××•\"×¤": "R&D",
    "×¤×™×ª×•×—": "R&D",
    "××©××‘×™ ×× ×•×©": "HR",
    "××©××‘×™-×× ×•×©": "HR",
    "××›×™×¨×•×ª ×•×©×™×¨×•×ª": "Sales & Service",
    "×©×™×¨×•×ª": "Sales & Service"
}

# ==========================================
# ×©××™×œ×ª×ª ×ª××™××•×ª ×œ-UI ×”×§×™×™× (View Pattern)
# ==========================================
def get_unified_data(conn):
    """××™×™×¦×¨ ××ª ×”×˜×‘×œ×” ×”×©×˜×•×—×” ×©×”×“×©×‘×•×¨×“ ×©×œ× ×• ××›×™×¨ ××ª×•×š 3 ×”×˜×‘×œ××•×ª ×”××§×•×©×¨×•×ª"""
    query = '''
        SELECT
            c.name as candidate_name,
            c.email,
            c.source,
            j.job_title,
            j.department,
            a.status,
            a.recruiter,
            a.start_date,
            a.days_in_process,
            a.upload_log_id
        FROM applications a
        JOIN candidates c ON a.candidate_id = c.id
        JOIN jobs j ON a.job_id = j.id
    '''
    return pd.read_sql(query, conn)


@app.get("/")
def read_root():
    return {"status": "Phoenix Enterprise Brain is Active ğŸ§ "}


# ==========================================
# 2. ×× ×•×¢ ×”-ETL (×§×œ×™×˜×”, × ×™×§×•×™, ×—×œ×•×§×” ×œ×˜×‘×œ××•×ª)
# ==========================================
@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    log_id = str(uuid.uuid4())[:8]
    temp_file = f"temp_{file.filename}"

    try:
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # Extract
        try:
            df = pd.read_csv(temp_file)
        except Exception:
            try:
                df = pd.read_csv(temp_file, encoding='iso-8859-8')
            except Exception:
                df = pd.read_excel(temp_file)

        # ×–×™×”×•×™ ×¢××•×“×•×ª ×“×™× ××™ (×ª×•××š ×‘×›××” ×•×¨×™××¦×™×•×ª ×©×œ ×©××•×ª ××”××§×¡×œ)
        df.columns = df.columns.str.strip()
        col_map = {
            '×©× ××•×¢××“': 'name', '×©×': 'name',
            '×“×•×"×œ': 'email', '××™××™×™×œ': 'email', '××™×™×œ': 'email',
            '×©× ×”××©×¨×”': 'job_title', '××©×¨×”': 'job_title',
            '××¦×‘ ×©×™×•×š ×œ××©×¨×”': 'status', '×¡×˜×˜×•×¡': 'status',
            '××’×™×™×¡': 'recruiter', '××’×™×™×¡×ª': 'recruiter',
            '×ª×—×™×œ×ª ×’×™×•×¡': 'start_date', '×ª××¨×™×š ×¤×ª×™×—×”': 'start_date',
            '×¨××” 2': 'department', '××—×œ×§×”': 'department', '×—×˜×™×‘×”': 'department',
            '××§×•×¨ ×”×’×¢×”': 'source', '××§×•×¨': 'source'
        }
        df.rename(columns=col_map, inplace=True)

        # Transform: ×˜×™×¤×•×œ ×‘×—×•×¡×¨×™× (Data Imputation)
        if 'name' not in df.columns:
            raise Exception("×—×•×‘×” ×œ×›×œ×•×œ ×¢××•×“×ª ×©× ××•×¢××“")
        if 'job_title' not in df.columns:
            raise Exception("×—×•×‘×” ×œ×›×œ×•×œ ×¢××•×“×ª ×©× ××©×¨×”")
        if 'email' not in df.columns:
            df['email'] = df['name'].apply(lambda x: f"{str(x).replace(' ', '.')}@unknown.com")
        if 'source' not in df.columns:
            df['source'] = "Organic / Unknown"
        if 'start_date' not in df.columns:
            df['start_date'] = pd.Timestamp.now()
        if 'department' not in df.columns:
            df['department'] = "General"
        if 'status' not in df.columns:
            df['status'] = "×—×“×©"
        if 'recruiter' not in df.columns:
            df['recruiter'] = "×œ× ×©×•×™×š"

        # Transform: × ×•×¨××œ×™×–×¦×™×”
        df['department'] = df['department'].replace(DEPT_NORMALIZATION)
        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce').fillna(pd.Timestamp.now())
        df['days_in_process'] = (pd.Timestamp.now() - df['start_date']).dt.days.fillna(0).astype(int)

        # Load: ×¤×ª×™×—×ª ×—×™×‘×•×¨ ×œ××¡×“ ×”× ×ª×•× ×™× ×•×”×–×¨×§×ª × ×ª×•× ×™× ×œ×˜×‘×œ××•×ª × ×¤×¨×“×•×ª
        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()

        rows_processed = 0

        for _, row in df.iterrows():
            # 1. ×˜×™×¤×•×œ ×‘××•×¢××“ (Upsert)
            c_id = str(uuid.uuid5(uuid.NAMESPACE_URL, str(row['email'])))  # ××–×”×” ×§×‘×•×¢ ×œ×¤×™ ××™××™×™×œ
            c.execute("INSERT OR IGNORE INTO candidates (id, name, email, source) VALUES (?, ?, ?, ?)",
                      (c_id, str(row['name']), str(row['email']), str(row['source'])))

            # 2. ×˜×™×¤×•×œ ×‘××©×¨×” (Insert OR Ignore)
            j_id = str(uuid.uuid5(uuid.NAMESPACE_URL, str(row['job_title'])))
            c.execute("INSERT OR IGNORE INTO jobs (id, job_title, department) VALUES (?, ?, ?)",
                      (j_id, str(row['job_title']), str(row['department'])))

            # 3. ×˜×™×¤×•×œ ×‘×ª×”×œ×™×š (Upsert ×œ×¤×™ ××•×¢××“+××©×¨×”)
            app_id = f"{c_id}_{j_id}"
            c.execute("SELECT status FROM applications WHERE app_id = ?", (app_id,))
            exists = c.fetchone()

            if exists:
                # ×¢×“×›×•×Ÿ ×¡×˜×˜×•×¡ ×œ×ª×”×œ×™×š ×§×™×™×
                c.execute('''UPDATE applications SET status = ?, recruiter = ?, days_in_process = ?, upload_log_id = ?
                             WHERE app_id = ?''',
                          (str(row['status']), str(row['recruiter']), int(row['days_in_process']), log_id, app_id))
            else:
                # ×ª×”×œ×™×š ×—×“×©
                c.execute('''INSERT INTO applications (app_id, candidate_id, job_id, status, recruiter, start_date, days_in_process, upload_log_id)
                             VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                          (app_id, c_id, j_id, str(row['status']), str(row['recruiter']), row['start_date'].strftime('%Y-%m-%d'), int(row['days_in_process']), log_id))

            rows_processed += 1

        # ×¨×™×©×•× ×‘×™×•××Ÿ
        c.execute("INSERT INTO data_logs (log_id, filename, upload_date, rows_processed, status) VALUES (?, ?, ?, ?, ?)",
                  (log_id, file.filename, pd.Timestamp.now().strftime("%Y-%m-%d %H:%M"), rows_processed, "Success"))

        conn.commit()
        conn.close()
        os.remove(temp_file)

        return {"message": "ETL Completed successfully", "rows_processed": rows_processed}

    except Exception as e:
        if os.path.exists(temp_file):
            os.remove(temp_file)
        raise HTTPException(status_code=500, detail=str(e))


# ==========================================
# 3. DATA GOVERNANCE API (Admin Tools)
# ==========================================
@app.get("/admin/health")
def get_data_health():
    """×—×™×©×•×‘ ×‘×¨×™××•×ª × ×ª×•× ×™× ××©×•×§×œ×œ ×¢×œ ×¤× ×™ ×”×˜×‘×œ××•×ª"""
    conn = sqlite3.connect(DB_PATH)
    try:
        c = conn.cursor()
        c.execute("SELECT COUNT(*) FROM candidates")
        total_candidates = c.fetchone()[0]
        c.execute("SELECT COUNT(*) FROM applications")
        total_apps = c.fetchone()[0]

        # ××¦×™××ª ×—×•×¡×¨×™×
        c.execute("SELECT COUNT(*) FROM applications WHERE recruiter = '×œ× ×©×•×™×š' OR recruiter IS NULL")
        missing_rec = c.fetchone()[0]
        c.execute("SELECT COUNT(*) FROM jobs WHERE department = 'General' OR department IS NULL")
        missing_dept = c.fetchone()[0]

        logs_df = pd.read_sql("SELECT * FROM data_logs ORDER BY upload_date DESC LIMIT 10", conn)

        health_score = 100
        if total_apps > 0:
            health_score = max(0, int(100 - (((missing_rec + missing_dept) / (total_apps + total_candidates)) * 100)))

        missing_data = []
        if missing_rec > 0:
            missing_data.append({"field": "×ª×”×œ×™×›×™× ×œ×œ× ××’×™×™×¡", "count": missing_rec})
        if missing_dept > 0:
            missing_data.append({"field": "××©×¨×•×ª ×œ×œ× ×©×™×•×š ××—×œ×§×ª×™", "count": missing_dept})

        return {
            "health_score": health_score,
            "total_records": total_apps,
            "missing_data": missing_data,
            "logs": logs_df.to_dict(orient="records")
        }
    finally:
        conn.close()


@app.post("/admin/revert/{log_id}")
def revert_upload(log_id: str):
    """××—×™×§×ª ×›×œ ×”× ×ª×•× ×™× ×©× ×•×¦×¨×• ×¢×œ ×™×“×™ ×§×•×‘×¥ ××¡×•×™× (Rollback)"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        # ××•×—×§ ×ª×”×œ×™×›×™× ×©× ×•×¦×¨×• ×‘×”×¢×œ××” ×–×•
        c.execute("DELETE FROM applications WHERE upload_log_id = ?", (log_id,))
        # ××¢×“×›×Ÿ ××ª ×”×¡×˜×˜×•×¡ ×‘×™×•××Ÿ
        c.execute("UPDATE data_logs SET status = 'Reverted' WHERE log_id = ?", (log_id,))
        # (×‘×•× ×•×¡ ×¢×ª×™×“×™: ×œ× ×§×•×ª ××•×¢××“×™× "×™×ª×•××™×" ×©××™×Ÿ ×œ×”× ×™×•×ª×¨ ×ª×”×œ×™×›×™×)
        conn.commit()
        return {"message": f"Upload {log_id} has been reverted successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        conn.close()


# ==========================================
# 4. DASHBOARD API (Endpoints for the UI)
# ==========================================
@app.get("/meta")
def get_meta():
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
        departments = [d for d in df['department'].dropna().unique().tolist() if str(d).strip()]
        recruiters = [r for r in df['recruiter'].dropna().unique().tolist() if str(r).strip()]
        return {"departments": sorted(departments), "recruiters": sorted(recruiters)}
    except Exception:
        return {"departments": [], "recruiters": []}
    finally:
        conn.close()


@app.get("/stats")
def get_stats(timeframe: str = "all", department: str = "all", recruiter: str = "all"):
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
        df['start_date'] = pd.to_datetime(df['start_date'])
    except Exception:
        return {"total_candidates": 0, "hired_this_month": 0, "avg_days": 0, "sla_alerts": 0, "chart_data": []}
    finally:
        conn.close()

    if df.empty:
        return {"total_candidates": 0, "hired_this_month": 0, "avg_days": 0, "sla_alerts": 0, "chart_data": []}

    # ×”×¤×¢×œ×ª ×¡×™× ×•× ×™× ×—×›××™× (Slicers)
    if department != "all":
        df = df[df['department'] == department]
    if recruiter != "all":
        df = df[df['recruiter'] == recruiter]
    if timeframe == "30days":
        df = df[df['start_date'] >= (pd.Timestamp.now() - pd.Timedelta(days=30))]
    elif timeframe == "year":
        df = df[df['start_date'].dt.year == pd.Timestamp.now().year]

    closed_statuses = ['×§×œ×™×˜×”', '×’×™×•×¡', '×“×—×™×™×”', '×”×¡×¨×”', '×•×™×ª×•×¨', '×”×§×¤××”']
    df['is_active'] = ~df['status'].str.contains('|'.join(closed_statuses), case=False, na=False)

    recent_df = df[df['start_date'].dt.year >= (pd.Timestamp.now().year - 1)].copy() if timeframe == "all" else df.copy()

    total = len(df)
    current_month = pd.Timestamp.now().month
    hired_df = recent_df[(recent_df['status'].str.contains('×§×œ×™×˜×”|×’×™×•×¡', case=False, na=False)) & (recent_df['start_date'].dt.month == current_month)]
    hired_count = len(hired_df)

    all_hired = recent_df[recent_df['status'].str.contains('×§×œ×™×˜×”|×’×™×•×¡', case=False, na=False)]
    avg_days = int(all_hired['days_in_process'].mean()) if not all_hired.empty else 0

    # ×—×™×©×•×‘ SLA ××“×¤×˜×™×‘×™ (×œ×¤×™ ×¡×•×’ ××©×¨×” - ××•×§×“×™× ××•×œ ××˜×”/×˜×›× ×•×œ×•×’×™)
    active_df = recent_df[recent_df['is_active']]
    sla_count = len(active_df[
        ((active_df['department'].str.contains('×©×™×¨×•×ª|××›×™×¨×•×ª|××•×§×“×™×', case=False, na=False)) & (active_df['days_in_process'] > 29)) |
        ((~active_df['department'].str.contains('×©×™×¨×•×ª|××›×™×¨×•×ª|××•×§×“×™×', case=False, na=False)) & (active_df['days_in_process'] > 44))
    ])

    graph_df = recent_df.copy()
    graph_df['month_name'] = graph_df['start_date'].dt.strftime('%b')
    graph_df['month_num'] = graph_df['start_date'].dt.month
    chart_data = graph_df.groupby(['month_num', 'month_name']).size().reset_index(name='candidates').sort_values('month_num')
    formatted_chart = [{"name": row['month_name'], "candidates": int(row['candidates'])} for _, row in chart_data.iterrows()]

    return {"total_candidates": total, "hired_this_month": hired_count, "avg_days": avg_days, "sla_alerts": sla_count, "chart_data": formatted_chart}


@app.get("/candidates")
def get_candidates(page: int = 1, limit: int = 50, search: str = "", sort_by: str = "days_in_process", sort_dir: str = "desc"):
    offset = (page - 1) * limit
    conn = sqlite3.connect(DB_PATH)

    try:
        df = get_unified_data(conn)
    except Exception:
        return {"data": [], "page": page, "total": 0}
    finally:
        conn.close()

    if df.empty:
        return {"data": [], "page": page, "total": 0}

    # 1. ×—×™×¤×•×©
    if search:
        mask = (
            df['candidate_name'].str.contains(search, case=False, na=False) |
            df['job_title'].str.contains(search, case=False, na=False) |
            df['recruiter'].str.contains(search, case=False, na=False)
        )
        df = df[mask]

    # 2. ××™×•×Ÿ ×—×›×
    valid_columns = {
        "candidate_name": "candidate_name",
        "job_title": "job_title",
        "status": "status",
        "recruiter": "recruiter",
        "days_in_process": "days_in_process",
        "department": "department"
    }
    safe_sort_col = valid_columns.get(sort_by, "days_in_process")
    ascending = sort_dir.lower() == "asc"

    df = df.sort_values(safe_sort_col, ascending=ascending)

    total = len(df)
    df_page = df.iloc[offset:offset + limit]

    return {"data": df_page.to_dict(orient="records"), "page": page, "total": total}


@app.get("/jobs")
def get_jobs():
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
    except Exception:
        return []
    finally:
        conn.close()

    if df.empty:
        return []

    # ×”×’×“×¨×ª ×¡×˜×˜×•×¡×™× ×¡×•×¤×™×™× (×‘×“×™×•×§ ×›××• ×‘×“×©×‘×•×¨×“)
    closed_statuses = ['×§×œ×™×˜×”', '×’×™×•×¡', '×“×—×™×™×”', '×”×¡×¨×”', '×•×™×ª×•×¨', '×”×§×¤××”']
    df['is_active'] = ~df['status'].str.contains('|'.join(closed_statuses), case=False, na=False)

    # × ×™×§×— ×¨×§ ××•×¢××“×™× ×©×¢×“×™×™×Ÿ ×¤×¢×™×œ×™× ×‘×ª×”×œ×™×š
    active_df = df[df['is_active']]

    jobs_summary = []

    # ×§×™×‘×•×¥ ×œ×¤×™ ×©× ×”××©×¨×”
    for job_title, group in active_df.groupby('job_title'):
        active_candidates_count = len(group)
        avg_days = int(group['days_in_process'].mean())
        max_days = int(group['days_in_process'].max())
        sla_breaches = len(group[group['days_in_process'] > 40])

        department = group['department'].iloc[0] if pd.notna(group['department'].iloc[0]) else "×›×œ×œ×™"
        recruiter = group['recruiter'].iloc[0] if pd.notna(group['recruiter'].iloc[0]) else "×œ× ×©×•×™×š"

        jobs_summary.append({
            "job_title": job_title,
            "department": department,
            "recruiter": recruiter,
            "active_candidates": active_candidates_count,
            "avg_days": avg_days,
            "max_days": max_days,
            "sla_breaches": sla_breaches,
            "health": "danger" if sla_breaches > 2 else "warning" if sla_breaches > 0 else "good"
        })

    # ××™×•×Ÿ: ×¦×•×•××¨×™ ×‘×§×‘×•×§ ×œ××¢×œ×”
    jobs_summary.sort(key=lambda x: (x['sla_breaches'], x['max_days']), reverse=True)

    return jobs_summary


@app.get("/executive-brief")
def get_executive_brief():
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
        df['start_date'] = pd.to_datetime(df['start_date'])
    except Exception:
        return {"error": "No data"}
    finally:
        conn.close()

    if df.empty:
        return {"error": "No data"}

    closed_statuses = ['×§×œ×™×˜×”', '×’×™×•×¡', '×“×—×™×™×”', '×”×¡×¨×”', '×•×™×ª×•×¨', '×”×§×¤××”']
    df['is_active'] = ~df['status'].str.contains('|'.join(closed_statuses), case=False, na=False)
    active_df = df[df['is_active']]

    # Metrics
    total_active = len(active_df)
    sla_breaches = len(active_df[active_df['days_in_process'] > 40])

    current_month = pd.Timestamp.now().month
    current_year = pd.Timestamp.now().year
    hired_this_month = len(df[(df['status'].str.contains('×§×œ×™×˜×”|×’×™×•×¡', case=False, na=False)) &
                              (df['start_date'].dt.month == current_month) &
                              (df['start_date'].dt.year == current_year)])

    # ×—×™×©×•×‘ ×¦×•×•××¨×™ ×”×‘×§×‘×•×§ ×”××¨×›×–×™×™×
    bottlenecks = []
    for job_title, group in active_df.groupby('job_title'):
        breaches = len(group[group['days_in_process'] > 40])
        if breaches > 0:
            bottlenecks.append({
                "job": job_title,
                "breaches": breaches,
                "recruiter": str(group['recruiter'].iloc[0]) if pd.notna(group['recruiter'].iloc[0]) else "×œ× ××•×’×“×¨"
            })

    bottlenecks.sort(key=lambda x: x['breaches'], reverse=True)
    top_3 = bottlenecks[:3]

    # ×™×¦×™×¨×ª ×ª×•×‘× ×” ×—×›××” (Simulated AI Insight)
    if total_active > 0:
        breach_percentage = int((sla_breaches / total_active) * 100)
        if breach_percentage > 15:
            insight = f"âš ï¸ ×©×™××• ×œ×‘: {breach_percentage}% ××”×¦× ×¨×ª ×”×¤×¢×™×œ×” × ××¦××ª ×‘×—×¨×™×’×ª SLA (××¢×œ 40 ×™×•×). ×™×© ×œ××§×“ ××××¦×™ ×’×™×•×¡ ×‘×©×—×¨×•×¨ ×¦×•×•××¨×™ ×”×‘×§×‘×•×§ ×‘××©×¨×•×ª ×”××•×‘×™×œ×•×ª."
        elif hired_this_month > 10:
            insight = f"âœ… ×§×¦×‘ ×”×’×™×•×¡×™× ×”×—×•×“×© ××¢×•×œ×”. ×—×¨×™×’×•×ª ×”-SLA ×¢×•××“×•×ª ×¢×œ ×¨××” ×ª×§×™× ×” ×©×œ {breach_percentage}%."
        else:
            insight = f"â„¹ï¸ ×”×¦× ×¨×ª ×™×¦×™×‘×”. ×™×© ×œ×©×™× ×“×’×© ×¢×œ {len(top_3)} ×”××©×¨×•×ª ×”××¢×›×‘×•×ª ××ª ×”×××•×¦×¢ ×”××¨×’×•× ×™."
    else:
        insight = "××™×Ÿ ××¡×¤×™×§ × ×ª×•× ×™× ×¤×¢×™×œ×™× ×œ×”×¤×§×ª ×ª×•×‘× ×•×ª."

    return {
        "date": pd.Timestamp.now().strftime("%d/%m/%Y"),
        "total_active": total_active,
        "hired_this_month": hired_this_month,
        "sla_breaches": sla_breaches,
        "top_bottlenecks": top_3,
        "insight": insight
    }


@app.get("/intelligence")
def get_intelligence():
    """×× ×•×¢ ×”×¤×§×ª ×ª×•×‘× ×•×ª, ××©×¤×›×™×, ×•×¨×“××¨ ×¡×™×›×•× ×™× ××”×“××˜×” ×”×××™×ª×™"""
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
    except Exception:
        return {"error": "No data"}
    finally:
        conn.close()

    if df.empty:
        return {"error": "No data"}

    # --- 1. ××©×¤×š ×”××¨×” ×“×™× ××™ ××‘×•×¡×¡ × ×ª×•× ×™× ×××™×ª×™×™× (Real Funnel) ---
    total_candidates = len(df)

    # ×—×™×¤×•×© ×˜×§×¡×˜×•××œ×™ ×—×›× ×©×œ ×”×¡×˜×˜×•×¡×™×
    cv_review = total_candidates  # ×›×•×œ× ××ª×—×™×œ×™× ×¤×”
    phone_screen = len(df[df['status'].str.contains('×˜×œ×¤×•× ×™|×¨××©×•× ×™|×¨××™×•×Ÿ HR|×× ×”×œ', case=False, na=False)])
    interviews = len(df[df['status'].str.contains('×¨××™×•×Ÿ HR|××©××‘×™ ×× ×•×©|×¨××™×•×Ÿ ×× ×”×œ|××§×¦×•×¢×™|××¨×›×– ×”×¢×¨×›×”', case=False, na=False)])
    offers = len(df[df['status'].str.contains('×”×¦×¢×ª ×©×›×¨|×—×•×–×”|×××ª×™×Ÿ ×œ×—×ª×™××”', case=False, na=False)])
    hired = len(df[df['status'].str.contains('×§×œ×™×˜×”|×’×™×•×¡', case=False, na=False)])

    # ×™×¦×™×¨×ª ×”××‘× ×” ×©×”×¤×¨×•× ×˜×× ×“ ××¦×¤×” ×œ×•
    funnel = [
        {"stage": "×§×•×¨×•×ª ×—×™×™× (Sourcing)", "count": cv_review, "percentage": 100},
        {"stage": "×¡×™× ×•×Ÿ ×¨××©×•× ×™ / ×˜×œ×¤×•× ×™", "count": phone_screen, "percentage": int((phone_screen / cv_review) * 100) if cv_review > 0 else 0},
        {"stage": "×¨××™×•× ×•×ª (HR + ××§×¦×•×¢×™)", "count": interviews, "percentage": int((interviews / cv_review) * 100) if cv_review > 0 else 0},
        {"stage": "×”×¦×¢×•×ª ×©×›×¨", "count": offers, "percentage": int((offers / cv_review) * 100) if cv_review > 0 else 0},
        {"stage": "×§×œ×™×˜×•×ª ×‘×¤×•×¢×œ", "count": hired, "percentage": int((hired / cv_review) * 100) if cv_review > 0 else 0}
    ]

    # --- 2. ×¨×“××¨ × ×˜×™×©×” (Ghosting Predictor) ×××™×ª×™ ---
    closed_statuses = ['×§×œ×™×˜×”', '×’×™×•×¡', '×“×—×™×™×”', '×”×¡×¨×”', '×•×™×ª×•×¨', '×”×§×¤××”']
    df['is_active'] = ~df['status'].str.contains('|'.join(closed_statuses), case=False, na=False)
    active_df = df[df['is_active']]

    # ××•×¢××“×™× ×¤×¢×™×œ×™× ×©×ª×§×•×¢×™× ××¢×œ 14 ×™×•× ×‘×œ×™ ×ª×–×•×–×”
    risk_df = active_df[active_df['days_in_process'] > 14].sort_values('days_in_process', ascending=False).head(8)

    ghosting_risks = []
    for _, row in risk_df.iterrows():
        # ×›×›×œ ×©×”×™××™× ×¢×•×œ×™× ××¢×œ 14, ×”-Risk Score ××–× ×§ ×¢×“ 99%
        prob = min(99, int(40 + (row['days_in_process'] - 14) * 3))
        ghosting_risks.append({
            "candidate": row['candidate_name'],
            "job": row['job_title'],
            "days": int(row['days_in_process']),
            "risk_score": prob,
            "recruiter": row['recruiter'] if pd.notna(row['recruiter']) else "×œ× ×©×•×™×š"
        })

    baseline_days = int(active_df['days_in_process'].mean()) if not active_df.empty else 0

    return {
        "funnel": funnel,
        "ghosting_risks": ghosting_risks,
        "baseline": {
            "avg_days": baseline_days,
            "current_hires": hired
        }
    }


@app.get("/drilldown")
def get_drilldown(month_name: str, timeframe: str = "all", department: str = "all", recruiter: str = "all"):
    """×©×•×œ×£ ××ª ×¨×©×™××ª ×”××•×¢××“×™× ×”××“×•×™×§×ª ×©×œ ×—×•×“×© ×¡×¤×¦×™×¤×™ (×œ×¤×™ ×—×™×ª×•×›×™×)"""
    conn = sqlite3.connect(DB_PATH)
    try:
        df = get_unified_data(conn)
        df['start_date'] = pd.to_datetime(df['start_date'])
    except Exception:
        return []
    finally:
        conn.close()

    if df.empty:
        return []

    # --- ××¤×¢×™×œ×™× ××ª ××•×ª× ×¡×™× ×•× ×™× ××”×“×©×‘×•×¨×“ ×”×¨××©×™ ---
    if department != "all":
        df = df[df['department'] == department]
    if recruiter != "all":
        df = df[df['recruiter'] == recruiter]

    if timeframe == "30days":
        df = df[df['start_date'] >= (pd.Timestamp.now() - pd.Timedelta(days=30))]
    elif timeframe == "year":
        df = df[df['start_date'].dt.year == pd.Timestamp.now().year]

    # --- ×—×™×ª×•×š ×¡×¤×¦×™×¤×™ ×œ×—×•×“×© ×©× ×œ×—×¥ ×‘×’×¨×£ ---
    df['month_short'] = df['start_date'].dt.strftime('%b')
    df_month = df[df['month_short'] == month_name].copy()

    if df_month.empty:
        return []

    df_month = df_month.sort_values('days_in_process', ascending=False)

    records = df_month[['candidate_name', 'job_title', 'status', 'recruiter', 'days_in_process']].fillna("").to_dict(orient="records")
    return records


@app.get("/admin/costs")
def get_costs():
    """×¡×™××•×œ×¦×™×” ×©×œ × ×ª×•× ×™ ×›×¡×¤×™×, ×”×¡×›××™× ×•×¢×œ×•×™×•×ª ×’×™×•×¡ (CPH)"""
    return {
        "cph_average": "â‚ª7,820",
        "total_spend_ytd": "â‚ª265,000",
        "agencies": [
            {"name": "×—×‘×¨×•×ª ×”×©××” (×˜×›× ×•×œ×•×’×™×”)", "type": "×¢××œ×”", "value": "100%", "active": 45, "hired": 12, "est_cost": "â‚ª180,000", "roi": "×’×‘×•×”"},
            {"name": "LinkedIn Recruiter", "type": "×¨×™×©×™×•×Ÿ ×©× ×ª×™", "value": "â‚ª45,000", "active": 120, "hired": 8, "est_cost": "â‚ª45,000", "roi": "×‘×™× ×•× ×™"},
            {"name": "×—×‘×¨ ××‘×™× ×—×‘×¨ (Referral)", "type": "×‘×•× ×•×¡ ×”×•×§×¨×”", "value": "â‚ª3,000", "active": 80, "hired": 14, "est_cost": "â‚ª42,000", "roi": "×’×‘×•×” ×××•×“"},
            {"name": "×§××¤×™×™× ×™× ×××•×× ×™× (Facebook/IG)", "type": "×ª×§×¦×™×‘ ×—×•×“×©×™", "value": "â‚ª2,500", "active": 210, "hired": 3, "est_cost": "â‚ª12,500", "roi": "× ××•×š"}
        ]
    }


@app.get("/admin/automations")
def get_automations():
    """×©×œ×™×¤×ª ×—×•×§×™ ×”××•×˜×•××¦×™×” ×©××•×’×“×¨×™× ×‘××¢×¨×›×ª"""
    return [
        {"id": 1, "trigger": "×¡×˜×˜×•×¡ = '×”×¦×¢×ª ×©×›×¨'", "condition": "××¢×œ 3 ×™××™×", "action": "×©×œ×— ×”×ª×¨××” ××“×•××” ×œ×× ×”×œ ×”××’×™×™×¡", "status": "×¤×¢×™×œ"},
        {"id": 2, "trigger": "××§×•×¨ = '×—×‘×¨ ××‘×™× ×—×‘×¨'", "condition": "××¢×‘×¨ ×œ×¡×˜×˜×•×¡ '×§×œ×™×˜×”'", "action": "×”×•×¦× ××™×™×œ ×œ××“×•×¨ ×©×›×¨ ×œ×ª×©×œ×•× ×‘×•× ×•×¡", "status": "×¤×¢×™×œ"},
        {"id": 3, "trigger": "×ª×’×™×ª '×˜××œ× ×˜' × ×•×¡×¤×”", "condition": "××™×Ÿ ××™× ×˜×¨××§×¦×™×” 14 ×™×•×", "action": "×”×§×¤×¥ ×œ××’×™×™×¡×ª ×ª×–×›×•×¨×ª (Nudge)", "status": "×¤×¢×™×œ"},
        {"id": 4, "trigger": "×—×˜×™×‘×ª ×˜×›× ×•×œ×•×’×™×”", "condition": "××¢×œ 60 ×™××™× ×‘'×¨××™×•×Ÿ ××§×¦×•×¢×™'", "action": "×“×•×•×— ×›×—×¨×™×’×ª SLA ×—××•×¨×”", "status": "××•×©×”×”"}
    ]


# ==========================================
# 4. FINOPS & BUDGET API (× ×™×”×•×œ ×ª×§×¦×™×‘)
# ==========================================

@app.get("/api/finops/data")
def get_finops_data():
    conn = sqlite3.connect(DB_PATH)
    try:
        categories_df = pd.read_sql("SELECT * FROM finops_categories", conn)
        categories = categories_df.to_dict(orient="records")
        for cat in categories:
            cat['subcategories'] = json.loads(cat['subcategories']) if cat['subcategories'] else []

        vendors_df = pd.read_sql("SELECT * FROM finops_vendors", conn)
        invoices_df = pd.read_sql("SELECT * FROM finops_invoices ORDER BY date DESC", conn)

        return {
            "categories": categories,
            "vendors": vendors_df.to_dict(orient="records"),
            "invoices": invoices_df.to_dict(orient="records")
        }
    except Exception as e:
        return {"error": str(e), "categories": [], "vendors": [], "invoices": []}
    finally:
        conn.close()

@app.post("/api/finops/upload_invoice")
async def upload_invoice(file: UploadFile = File(...)):
    """××§×‘×œ ×§×•×‘×¥ PDF/×ª××•× ×” ×©×œ ×—×©×‘×•× ×™×ª, ×©×•××¨ ××•×ª×• ×•××—×–×™×¨ × ×ª×•× ×™× ×¨××©×•× ×™×™×"""
    os.makedirs("uploads/invoices", exist_ok=True)
    file_path = f"uploads/invoices/{uuid.uuid4().hex[:8]}_{file.filename}"
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    extracted_data = {
        "id": f"INV-{uuid.uuid4().hex[:6].upper()}",
        "vendor": "×¡×¤×§ ×œ× ××–×•×”×” (×–×™×”×•×™ AI)",
        "amount": 0,
        "date": pd.Timestamp.now().strftime("%d/%m/%Y"),
        "category": "×›×œ×œ×™ ×œ××™×¤×•×™",
        "subcategory": "××—×¨",
        "status": "×××ª×™×Ÿ ×œ××™×¤×•×™",
        "file_url": file_path
    }
    
    return {"message": "Invoice processed", "extracted_data": extracted_data}

@app.post("/api/finops/save_invoice")
def save_invoice(invoice: dict):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''INSERT OR REPLACE INTO finops_invoices 
                     (id, vendor, date, due_date, budget_month, amount, category, subcategory, status, note, file_url) 
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                  (invoice['id'], invoice['vendor'], invoice['date'], invoice.get('dueDate', ''), 
                   invoice.get('budgetMonth', ''), invoice['amount'], invoice['category'], 
                   invoice.get('subcategory', ''), invoice['status'], invoice.get('note', ''), invoice.get('fileUrl', '')))
        conn.commit()
        return {"message": "Invoice saved"}
    finally:
        conn.close()

@app.delete("/api/finops/invoice/{invoice_id}")
def delete_invoice(invoice_id: str):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute("DELETE FROM finops_invoices WHERE id = ?", (invoice_id,))
        conn.commit()
        return {"message": "Deleted"}
    finally:
        conn.close()

@app.post("/api/finops/save_vendor")
def save_vendor(vendor: dict):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute('''INSERT OR REPLACE INTO finops_vendors (id, name, default_category, total_paid, active_invoices)
                     VALUES (?, ?, ?, ?, ?)''', 
                  (vendor['id'], vendor['name'], vendor.get('defaultCategory', ''), vendor.get('totalPaid', 0), vendor.get('activeInvoices', 0)))
        conn.commit()
        return {"message": "Vendor saved"}
    finally:
        conn.close()

@app.post("/api/finops/save_categories")
def save_categories(categories: list):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute("DELETE FROM finops_categories")
        for cat in categories:
            subs = json.dumps(cat.get('subcategories', []))
            c.execute('''INSERT INTO finops_categories (id, name, target, previous_year_spend, code, notes, subcategories)
                         VALUES (?, ?, ?, ?, ?, ?, ?)''', 
                      (cat['id'], cat['name'], cat.get('target', 0), cat.get('previousYearSpend', 0), cat.get('code', ''), cat.get('notes', ''), subs))
        conn.commit()
        return {"message": "Categories synced"}
    finally:
        conn.close()